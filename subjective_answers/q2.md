## L2 regularised
Accuracy:  83.33333333333334

### Cross validation

<pre>
<code>
______
lambda: 0.25   
Accuracy:  80.0
______
______
lambda: 0.5
Accuracy:  50.0
______
______
lambda: 0.75
Accuracy:  56.666666666666664
______
______
lambda: 1.0
Accuracy:  53.333333333333336
______
______
lambda: 1.25
Accuracy:  56.666666666666664
______
______
lambda: 1.5
Accuracy:  60.0
______
______
lambda: 1.75
Accuracy:  63.33333333333333
______
______
lambda: 2.0
Accuracy:  66.66666666666666
______
Average accuracy: 60.83333333333333

</code>
</pre>


## L1 Regularised

<pre>
<code>
______
lambda: 0.25
Accuracy:  76.66666666666667
______
______
lambda: 0.5
Accuracy:  43.333333333333336
______
______
lambda: 0.75
Accuracy:  56.666666666666664
______
______
lambda: 1.0
Accuracy:  46.666666666666664
______
______
lambda: 1.25
Accuracy:  60.0
______
______
lambda: 1.5
Accuracy:  56.666666666666664
______
______
lambda: 1.75
Accuracy:  70.0
______
______
lambda: 2.0
Accuracy:  70.0
______
Average accuracy: 60.0

</code>
</pre>


### L1 regularised weights:
<pre>
<code>
0    0.455883 
1    0.018523 
2    1.395462 
dtype: float64

</code>
</pre>

Accuracy:  76.66666666666667

Hence, more important feature is feature 2